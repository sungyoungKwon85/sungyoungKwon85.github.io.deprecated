---
layout: post
title:  "crawler"
date:   2017-02-17 13:10:53 +0900
categories: crawler
---
웹 크롤러에 대한 다양한 블로그를 읽고 메모한다.  

참고 :  
https://dobest.io/nodejs-web-crawling-with-cheerio/  

이글에서는 크롤링할 사이트로 개인 블로그의 메인 페이지에서 포스트 제목을 크로링 해본다.  

**데이터 수집하기 Crawling, Scraing, Parsing**  
1. Scraping - 어떻게 데이터를 잘 가져올 것인가?  
  - Authentication - 인증된 사용자만 정보를 볼 수 있는 경우?  
  - Pagination - 페이징 되어 있다면?  

2. Parsing - 데이터를 어떻게 잘 추출할 것인가?  

어떤 프로그래밍 언어를 쓰던지 Crawing, Scraping, Parsing에 적합한 각각의 모듈/패키지를 활용하면 된다.  
Node.js의 경우..  
- Scraping - http, https, request  
- Parsing - JSDOM, cheerio  

<br><br>



<br><br>
<br><br>
<br><br>
<br><br>
<br><br>
<br><br>
<br><br>



참고 :  
https://jiafei427.wordpress.com/2015/08/04/%EA%B0%95%EC%A2%8C-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%A4%EC%A0%84-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/  

데이터를 어떻게 얻을 것인가?  
**크롤러 : 웹의 데이터를 수집하는 프로그램**  

수집대상인 데이터의 형태는 정형/비정형 데이터로 구분  

REST API를 제공하는 서비스들은 XML, JSON, Binary 형태로 데이터를 제공, 이를 정형적 데이터라 할 수 있다.  

HTML로 제공되 직접 파싱해야 하는 데이터를 비정형적 데이터라 할 수 있다.  

버즈니 사례  
국내 모든 홈쇼핑 상품을 모아서 보여주는 홈쇼핑모아라는 모바일 앱 서비스 회사  
주기적으로 리뷰할 수 있는 페이지 주소를 확인해 HTML을 파싱하여 리뷰 데이터를 수집하는 크롤러를 작성했다.  


**URI 알아내기**  
구글 개발자 모드에서 POST등의 요청과 form-data를 확인할 수 있다.  
파이썬으로 요청을 날려본다. 결과가 JSON으로 날라온다.  


**HTML 파싱하기**  
파서는 다양한 라이브러리가 있고, 유명한 BeautifulSoup4를 사용한다.  
https://github.com/haandol/review_crawler  
소스는 위 URL에서 확인가능하다.  

크롤러의 핵심은 구현보단 운영이다.  
데이터 정확성 검증, 쿼리 횟수 제한 문제 등등 다양한 문제들이 발생한다.  
